# -*- coding: utf-8 -*-
"""DataForecaster

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fU-t-qzgk2XxVLODyvk2TiQ_yiCqXYnz

This notebook is a project demonstrating data prediction using 3 linear regression models.

*   Linear Regression using L2 regularization or Ridge Regression
*   Gradient Boosting Regressors
*   Neural Network based approaches using 1D Convolutions, LSTM layers with a Huber loss

The data we are predicting will be Adjusted Close prices for BAE Systems stock.

The historical stock prices for BAE Systems were downloaded from https://finance.yahoo.com/quote/BAESY for (Sept 3 2014 - Sept 3, 2019).

**GOOGLE COLAB USERS:** Runtime --> Change Runtime Type --> Hardware Accelerator -->GPU 
This will reduce time on Neural Network cells.

**Import necessary packages**
*    NumPy
*    Pandas
*    Matplotlib
*    Scikit-learn
*    Keras
*    Tensorflow
"""

# Commented out IPython magic to ensure Python compatibility.
# Import necessary packages
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
# %matplotlib inline

import sklearn
from sklearn.linear_model import Ridge
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.preprocessing import MinMaxScaler

import tensorflow as tf
import keras
from keras.models import Sequential
from keras.layers import Conv1D, LSTM, Dense
from keras.callbacks import LearningRateScheduler
from tensorflow.keras.losses import Huber
from keras.optimizers import SGD

"""**Download RAW CSV from gihub repo**"""

# Download the data and store it
# In this example, BAE Systems stock, 5 years, stored as a csv on github. 
# IF YOU USE A CSV HOSTED ON GITHUB, YOU MUST USE THE LINK TO THE RAW FILE, NOT THE PAGE LINK

!wget -O BAESY.csv https://raw.githubusercontent.com/danieljtrujillo/DataForecaster/master/BAESY.csv?dl=1

"""*   Read data using Pandas.
*   Set independent variable to *Date*
*   Display the first 5 entries
"""

df = pd.read_csv('BAESY.csv')

df.set_index('Date', inplace=True)
df.head()

"""*    Display *Adjusted Closing Price* for the last 5 years with the *Date* on the x-axis and *Price* on the y-axis"""

df['Adj Close'].plot(label='BAESY', figsize=(16,8), title='Adjusted Closing Price', grid=True)

"""By leveraging 90 prices from the past to predict the price on the 91st day, we can create a training example less affected by noise, and founded in historical data.

# *N*-90

*    ***N*** = number of days of downloaded stock data

Create a **2D matrix** of training samples wherein each row is ***Price*** of 90 consecutive days in the past and the ***Price*** to predict is the current day.
"""

window_size = 90 # Allow us to look at 90 days into the past
# Prepare the data so that we have 90 day windows and predict what the next day should be

# Get indices of access for the data
num_samples = len(df) - window_size
indices = np.arange(num_samples).astype(np.int)[:,None] + np.arange(window_size + 1).astype(np.int)

data = df['Adj Close'].values[indices] # Create the 2D matrix of training samples

"""**CREATE 2D MATRIX WITH 91 COLUMNS**
*    columns 1-90 = training samples
*    column 91 = target variable (predicted price)
"""

X = data[:,:-1] # Each row represents 90 days in the past
y = data[:,-1] # Each output value represents the 91st day

"""Perform train/test split such that the first 80% of the prices is the training set and the last 20% of the prices is the test dataset. We will train on the training set and test the performance on the test set to see how well we can forecast the price for BAE Systems stock."""

# Train and test split
split_fraction = 0.8
ind_split = int(split_fraction * num_samples)
X_train = X[:ind_split]
y_train = y[:ind_split]
X_test = X[ind_split:]
y_test = y[ind_split:]

"""**Method #1 - Ridge Regression**

We will use the Ridge Regression method from Scikit-learn where we'll train and perform inference on the training and test data respectively. Let's also plot how it compares with the actual data for both training and test.
"""

# Train
ridge_model = Ridge()
ridge_model.fit(X_train, y_train)

# Infer
y_pred_train_ridge = ridge_model.predict(X_train)
y_pred_ridge = ridge_model.predict(X_test)

# Plot what it looks like for the training data
df_ridge = df.copy()
df_ridge.drop(['Open', 'High', 'Low', 'Close', 'Volume'], axis=1, inplace=True)
df_ridge = df_ridge.iloc[window_size:ind_split] # Past 90 days we don't know yet
df_ridge['Adj Close Train'] = y_pred_train_ridge[:-window_size]
df_ridge.plot(label='BAESY', figsize=(16,8), title='Adjusted Closing Price', grid=True)

"""The results are extremely similar, which could be indicative of overfitting, so we compare to our test data."""

# Same for the test
df_ridge = df.copy()
df_ridge.drop(['Open', 'High', 'Low', 'Close', 'Volume'], axis=1, inplace=True)
df_ridge = df_ridge.iloc[ind_split+window_size:] # Past 90 days we don't know yet
df_ridge['Adj Close Test'] = y_pred_ridge
df_ridge.plot(label='BAESY', figsize=(16,8), title='Adjusted Closing Price', grid=True)

"""Once again, extremely similar.

**Method #2 - Gradient Boosting Trees**

Apply the same methodology from Method #1 to Gradient Boosting Trees in Scikit-learn.
"""

# Model #2 - Gradient Boosting Trees
gb_model = GradientBoostingRegressor()
gb_model.fit(X_train, y_train)

# Infer
y_pred_train_gb = gb_model.predict(X_train)
y_pred_gb = gb_model.predict(X_test)

# Plot what it looks like for the training data
df_gb = df.copy()
df_gb.drop(['Open', 'High', 'Low', 'Close', 'Volume'], axis=1, inplace=True)
df_gb = df_gb.iloc[window_size:ind_split] # Past 90 days we don't know yet
df_gb['Adj Close Train'] = y_pred_train_gb[:-window_size]
df_gb.plot(label='BAESY', figsize=(16,8), title='Adjusted Closing Price', grid=True)

"""Once again, extremely similar. Review the test data."""

# Same for the test
df_gb = df.copy()
df_gb.drop(['Open', 'High', 'Low', 'Close', 'Volume'], axis=1, inplace=True)
df_gb = df_gb.iloc[ind_split+window_size:] # Past 90 days we don't know yet
df_gb['Adj Close Test'] = y_pred_gb
df_gb.plot(label='BAESY', figsize=(16,8), title='Adjusted Closing Price', grid=True)

"""The results here make sense. Since trees performing majority voting so there's no interpolation involved, we can see at points of high curvature, it doesn't do so well but the general trend is correct. It seemed when 10+ years of historical data was used, it would "chop the top" off of peaks.


**Method #3 - Using neural networks with 1D convolutional and LSTM layers**

The last method we'll use is a neural network based one using Keras / Tensorflow. We use a stack of Conv1D and LSTM layers, followed by Dense layers lastly followed by a linear layer to predict the stock price.
"""

# Model #3 - Using RNNs
keras_model = Sequential([
  Conv1D(filters=128, kernel_size=5, input_shape=(window_size, 1), strides=1, padding="causal", activation="tanh"),
  Conv1D(filters=128, kernel_size=5, strides=1, padding="causal", activation="tanh"),
  LSTM(128, return_sequences=True),
  LSTM(128),
  Dense(64, activation="tanh"),
  Dense(64, activation="tanh"),
  Dense(1)
])

keras_model.summary()

"""To find the optimal learning rate for the neural network, we will incrementally increase the learning rate at each epoch, record the loss for each then choose the learning rate with the smallest overall loss.

Also, neural networks learn better when the data is normalized so we will normalize the data to the [âˆ’1,1] range by using Scikit-learn's MinMaxScaler. Therefore, we need to find apply this scaling on all of the price data, then decompose it into the training and test sets again.
"""

# First figure out the right learning rate
lr_schedule = LearningRateScheduler(lambda epoch: 1e-8 * 10**(epoch / 20))
optimizer = SGD(lr=1e-8, momentum=0.9)
keras_model.compile(loss=Huber(),
                    optimizer=optimizer,
                    metrics=["mae"])
# Scale the data due to LSTM units
scaler = MinMaxScaler(feature_range=(-1, 1))
data_transform = scaler.fit_transform(df['Adj Close'].values[:,None])
data_transform = data_transform[indices]
X2 = data_transform[:,:-1]
y2 = data_transform[:,-1]
X_train_reshape = X2[:ind_split]
y_train_reshape = y2[:ind_split]
X_test_reshape = X2[ind_split:]
y_test_reshape = y2[ind_split:]

"""The quest for the optimal training rate shall now commence. Running on Google Colab will take ~5 minutes"""

history = keras_model.fit(X_train_reshape, y_train_reshape, batch_size=256, epochs=250, callbacks=[lr_schedule], validation_data=(X_test_reshape, y_test_reshape))

"""Plot the Learning Rate vs Testing Data loss. 

Past a learning rate of 1, the loss explodes so we will focus in before the big explosion.
"""

plt.semilogx(history.history['lr'], history.history['val_loss'])
plt.legend(['Learning Rate', 'Validation Loss'])

plt.semilogx(history.history['lr'][:140], history.history['loss'][:140])

"""We will use the learning rate of .1 to train the neural network. Predict the prices of the training and test data."""

# Observing the above graph, 0.1 seems to be the best learning rate
keras_model = Sequential([
  Conv1D(filters=128, kernel_size=5, input_shape=(window_size, 1), strides=1, padding="causal", activation="tanh"),
  Conv1D(filters=128, kernel_size=5, strides=1, padding="causal", activation="tanh"),
  LSTM(128, return_sequences=True),
  LSTM(128),
  Dense(64, activation="tanh"),
  Dense(64, activation="tanh"),
  Dense(1)
])


optimizer = SGD(lr=0.1, momentum=0.9)
keras_model.compile(loss=Huber(),
                    optimizer=optimizer,
                    metrics=["mae"])
history = keras_model.fit(X_train_reshape, y_train_reshape, batch_size=256, epochs=250, validation_data=(X_test_reshape, y_test_reshape))

"""After the values are predicted, reverse the normalization and plot the predictions with the same scale as the original values."""

y_pred_train_keras = scaler.inverse_transform(keras_model.predict(X_train_reshape, batch_size=256))
y_pred_keras = scaler.inverse_transform(keras_model.predict(X_test_reshape, batch_size=256))

"""Let's now plot what the training and test predictions look like and compare witht he original values."""

# Plot what it looks like for the training data
df_keras = df.copy()
df_keras.drop(['Open', 'High', 'Low', 'Close', 'Volume'], axis=1, inplace=True)
df_keras = df_keras.iloc[window_size:ind_split] # Past 90 days we don't know yet
df_keras['Adj Close Train'] = y_pred_train_keras[:-window_size]
df_keras.plot(label='BAESY', figsize=(16,8), title='Adjusted Closing Price', grid=True)

"""Again, extremely similar, now do the same for the test data."""

# Same for the test
df_keras = df.copy()
df_keras.drop(['Open', 'High', 'Low', 'Close', 'Volume'], axis=1, inplace=True)
df_keras = df_keras.iloc[ind_split+window_size:] # Past 90 days we don't know yet
df_keras['Adj Close Test'] = y_pred_keras
df_keras.plot(label='BAESY', figsize=(16,8), title='Adjusted Closing Price', grid=True)

"""The neural network approach seems to indicate what the trend would be like when the data is noise free, with the transitions between points of high curvature smoothed.

Plot all methods for training and test data to compare.

First, the training data.
"""

df_train = df.copy()
df_train.drop(['Open', 'High', 'Low', 'Close', 'Volume'], axis=1, inplace=True)
df_train = df_train.iloc[window_size:ind_split] # Past 90 days we don't know yet
# Add in all of our methods
df_train['Adj Close Train Ridge'] = y_pred_train_ridge[:-window_size]
df_train['Adj Close Train Gradient Boosting'] = y_pred_train_gb[:-window_size]
df_train['Adj Close Train LSTM'] = y_pred_train_keras[:-window_size]
# Plot the data now
df_train.plot(label='BAESY', figsize=(16,8), title='Adjusted Closing Price', grid=True)

"""Nice and similar. Next, the testing data."""

df_test = df.copy()
df_test.drop(['Open', 'High', 'Low', 'Close', 'Volume'], axis=1, inplace=True)
df_test = df_test.iloc[ind_split+window_size:] # Past 90 days we don't know yet
# Add in all of our methods
df_test['Adj Close Test Ridge'] = y_pred_ridge
df_test['Adj Close Test Gradient Boosting'] = y_pred_gb
df_test['Adj Close Test LSTM'] = y_pred_keras
# Plot the data now
df_test.plot(label='BAESY', figsize=(16,8), title='Adjusted Closing Price', grid=True)

"""*   The neural network based method is what the values would be when there's no presence of noise. 
*   The ridge regression based method seems to follow the trend the best with the noise modelled in. 
*   The gradient boosting method also follows the same trend, but because the method does not use interpolation, points of very high curvature tend to be the most inaccurate.

**So what happens if we don't have testing data to try out?**

Take the learned model that you have, start with the last N values in your data (in my case 90), and start predicting what the next values will be. With the next values, keep using the previous N values for your desired forecasting.

We already have our learned models above, so let's go ahead and do that.
"""

num_days = 30 # Let's try and forecast the next 30 days or two years

# Get the last window_size (90) days
data_seed = df['Adj Close'].values[-window_size:][None]
# Get the normalized data as well for the neural network
data_seed_norm = scaler.transform(data_seed)
input_values = {"ridge": data_seed, "gb": data_seed, "keras": data_seed_norm.copy()}
values = {"ridge": [], "gb": [], "keras": []}
for i in range(num_days): # For each day...
    # Predict the next price given the previous N prices
    values["ridge"].append(ridge_model.predict(input_values["ridge"])[0])
    values["gb"].append(gb_model.predict(input_values["gb"])[0])
    values["keras"].append(keras_model.predict(input_values["keras"][...,None])[0][0])

    # Dump the oldest price and put the newest price at the end
    for v in input_values:
        val = input_values[v]
        val = np.insert(val, -1, values[v][-1], axis=1)
        val = np.delete(val, 0, axis=1)
        input_values[v] = val.copy()

# Convert all to NumPy arrays
for v in input_values:
    values[v] = np.array(values[v])

# Unnormalize prices from NN approach
values["keras"] = scaler.inverse_transform(values["keras"][None])[0]

"""Plot the Forecasted Adjusted Closing Price (in this example I went with 30, though 10 seems optimal)"""

from datetime import timedelta, datetime
last_date = datetime.strptime(df.index[-1], '%Y-%m-%d')
df_forecast = pd.DataFrame()
df_forecast["Ridge"] = values["ridge"]
df_forecast["Gradient Boosting"] = values["gb"]
df_forecast["Keras LSTM"] = values["keras"]
df_forecast.index = pd.date_range(start=last_date, periods=num_days)
df_forecast.plot(label='BAESY', figsize=(16,8), title='Forecasted Adjusted Closing Price', grid=True)

"""If you project too far into the furture, the *Ridge* method starts following a repetitive high amplitude peak and valley pattern, where-as the *Gradient Boosting* doesn't present any noticeable pattern but eventually levels out, and the *LSTM* method steadily decreases in amplitude much like ripples"""